{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chandujakkampudi/FMML_COURSE_ASSIGNMENTS/blob/main/Module4_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyMhDmOed0RJ"
      },
      "source": [
        "# FOUNDATIONS OF MODERN MACHINE LEARNING, IIIT Hyderabad\n",
        "# Module 4: Perceptron and Gradient Descent\n",
        "## Lab 2: Introduction to Gradient Descent\n",
        "### Module Coordinator: Jashn Arora\n",
        "\n",
        "Gradient descent is a very important algorithm to understand, as it underpins many of the more advanced algorithms used in Machine Learning and Deep Learning.\n",
        "\n",
        "A brief overview of the algorithm is\n",
        "\n",
        "\n",
        "*   start with a random initialization of the solution.\n",
        "*   incrementally change the solution by moving in the direction of negative gradient of the objective function.\n",
        "*   repeat the previous step until some convergence criteria is met.\n",
        "\n",
        "The key equation for change in weight is:\n",
        "$$w^{k+1} \\leftarrow w^k - \\eta \\Delta J$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5OzL5jbnkO"
      },
      "source": [
        "# Importing the required libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random   \n",
        "\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQpDHGOAh0It"
      },
      "source": [
        "We can start be choosing coefficients for a second degree polynomial equation $(a x^2 + bx + c)$ that will distribute the data we will try to model.\n",
        "\n",
        "Let's define some random x data (inputs) we hope to predict y (outputs) of."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnbvlEbWcUtM"
      },
      "source": [
        "def eval_2nd_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a second degree polynomial,\n",
        "    given a specific x value.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "    \n",
        "    Returns:\n",
        "        y: The corresponding output y value for the second degree polynomial.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [1, 0, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a-Tzv5fclE2"
      },
      "source": [
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQr81EuciKhB"
      },
      "source": [
        "This is good, but we could improve on this by making things more realistic. You can add noise or **jitter** to the values so they can resemble real-world data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggni_nKPdFZ5"
      },
      "source": [
        "def eval_2nd_degree_jitter(coeffs, x, j):\n",
        "    \"\"\"\n",
        "    Function to return the noisy output of evaluating a second degree polynomial,\n",
        "    given a specific x value. Output values can be within [y − j, y + j].\n",
        "    \n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "        j: Jitter parameter, to introduce noise to output y.\n",
        "    \n",
        "    Returns:\n",
        "        y: The corresponding jittered output y value for the second degree polynomial.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * x\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    \n",
        "    interval = [y - j, y + j]\n",
        "    interval_min = interval[0]\n",
        "    interval_max = interval[1]\n",
        "    jit_val = random.random() * interval_max      # Generate a random number in range 0 to interval max \n",
        "    \n",
        "    while interval_min > jit_val:                 # While the random jitter value is less than the interval min,\n",
        "        jit_val = random.random() * interval_max  # it is not in the right range. Re-roll the generator until it \n",
        "                                                  # give a number greater than the interval min. \n",
        "    \n",
        "    return jit_val\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_2nd_degree_jitter(coeffs, x, 0.1)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFYv43vpe5Y4"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.title('Original data with jitter')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umByA5Ghi_gt"
      },
      "source": [
        "We will now build our predictive model, and optimize it with gradient descent and we will try to get as close to these values as possible.\n",
        "\n",
        "To get a quantifiable measure of how incorrect it is, we calculate the Mean Squared Error loss for the model. This is the mean value of the sum of the squared differences between the actual and predicted outputs.\n",
        "\n",
        "$$ E = \\frac{1}{n} \\sum_{i=0}^n (y_i - \\bar{y_i})^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGo9VtQDfG6F"
      },
      "source": [
        "def loss_mse(ys, y_bar):\n",
        "    \"\"\"\n",
        "    Calculates MSE loss.\n",
        "    \n",
        "    Args:\n",
        "        ys: training data labels\n",
        "        y_bar: prediction labels\n",
        "    \n",
        "    Returns: Calculated MSE loss.\n",
        "    \"\"\"\n",
        "\n",
        "    return sum((ys - y_bar) * (ys - y_bar)) / len(ys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIRquRB3kcZA"
      },
      "source": [
        "rand_coeffs = (random.randrange(-10, 10), random.randrange(-10, 10), random.randrange(-10, 10))\n",
        "y_bar = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'prediction')\n",
        "plt.title('Original data vs first prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbwBb4Ckomw"
      },
      "source": [
        "initial_model_loss = loss_mse(ys, y_bar)\n",
        "initial_model_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEcvjxbJa8cq"
      },
      "source": [
        "We can see that the loss is quite a large number. Let’s now see if we can improve on this fairly high loss metric by optimizing the model with gradient descent.\n",
        "\n",
        "We wish to improve our model. Therefore we want to alter its coefficients $a$, $b$ and $c$ to decrease the error. Therefore we require knowledge about how each coefficient affects the error. This is achieved by calculating the partial derivative of the loss function with respect to **each** of the individual coefficients."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhiloANqkSFc"
      },
      "source": [
        "def calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys): \n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        \n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "    \n",
        "    y_bars = eval_2nd_degree(rand_coeffs, hundred_xs)\n",
        "    \n",
        "    for x, y, y_bar in list(zip(hundred_xs, ys, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c)   # return calculated gradients as a a tuple of its 3 parts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN0jR2Dhkpjn"
      },
      "source": [
        "calc_grad = calc_gradient_2nd_poly(rand_coeffs, hundred_xs, ys)\n",
        "\n",
        "lr = 0.0001\n",
        "a_new = rand_coeffs[0] - lr * calc_grad[0]\n",
        "b_new = rand_coeffs[1] - lr * calc_grad[1]\n",
        "c_new = rand_coeffs[2] - lr * calc_grad[2]\n",
        "\n",
        "new_model_coeffs = (a_new, b_new, c_new)\n",
        "print(f\"New model coeffs: {new_model_coeffs}\")\n",
        "\n",
        "# update with these new coeffs:\n",
        "new_y_bar = eval_2nd_degree(new_model_coeffs, hundred_xs)\n",
        "updated_model_loss = loss_mse(ys, new_y_bar)\n",
        "\n",
        "print(f\"Now have smaller model loss: {updated_model_loss} vs {initial_model_loss}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rjqrqclk4BI"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original model')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'first prediction')\n",
        "plt.plot(xs, new_y_bar, 'b.', label = 'updated prediction')\n",
        "plt.title('Original model vs 1st prediction vs updated prediction with lower loss')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOzSlzJIfvid"
      },
      "source": [
        "We’re almost ready. The last step will be to perform gradient descent iteratively over a number of epochs (cycles or iterations.) With every epoch we hope to see an improvement in the form of lowered loss, and better model-fitting to the original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBkU4dRnlHKy"
      },
      "source": [
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr): \n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: a,b and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        lr: learning rate\n",
        "        \n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a,b,c respectively.\n",
        "    \n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "        \n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2        \n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = x * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] - lr * gradient_a\n",
        "    b_new = coeffs[1] - lr * gradient_b\n",
        "    c_new = coeffs[2] - lr * gradient_c\n",
        "    \n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "    \n",
        "    # update with these new coeffs:\n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "    \n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj6K6SXol_bi"
      },
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \"\"\"\n",
        "    Perform gradient descent for a second degree polynomial.\n",
        "    \n",
        "    Args:\n",
        "        epochs: number of iterations to perform of finding new coefficients and updatingt loss. \n",
        "        lr: specified learning rate\n",
        "        \n",
        "    Returns: Tuple containing (updated_model_loss, new_model_coeffs, new_y_bar predictions, saved loss updates)\n",
        "    \n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  # (updated_model_loss, new_model_coeffs, new_y_bar, saved loss updates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brk2qRFlmAQM"
      },
      "source": [
        "GD = gradient_descent(30000, 0.0003)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS2KZ6SxfnAI"
      },
      "source": [
        "This trained model is showing vast improvements after it’s full training cycle. We can examine further by inspecting its final predicted coefficients $a$, $b$ and $c$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efY8ehhvmCRz"
      },
      "source": [
        "print(f\"Final Coefficients predicted: {GD[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8PuwB87fjP5"
      },
      "source": [
        "Not too far off! A big improvement over the initial random model. Looking at the plot of the loss reduction over training offers further insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnswAURtmFBG"
      },
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(GD[3], 'b-', label = 'loss')\n",
        "plt.title('Loss over 500 iterations')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7fnsphdJpo"
      },
      "source": [
        "We observe that the model loss reached close to zero, to give us our more accurate coefficients. We can also see there was no major improvement in loss after about 100 epochs. An alternative strategy would be to add some kind of condition to the training step that stops training when a certain minimum loss threshold has been reached. This would prevent excessive training and potential over-fitting for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3oxAVdtePYa"
      },
      "source": [
        "# Things to try\n",
        "\n",
        "\n",
        "\n",
        "1.   Change the coefficients array and try a different polynomial instead of our $x^2$.\n",
        "2.   Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "3. Take a degree 5 polynomial with 5 roots and try different initializations, instead of random ones. Does it converge on different values for different initializations? Why does initialization not matter in our case of $x^2$?\n",
        "4. Can you modify the algorithm to find a maxima of a function, instead of a minima?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution for 1**\n",
        "\n",
        "here we are taking the equation as\n",
        "![Screenshot 2023-01-04 180743.png](data:image/png;base64,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)\n",
        "\n",
        "\n",
        "and will try to converge this equation\n",
        "\n",
        "**Observation**\n",
        "Here we will observe that this equation or the function sometimes converges as in this there will be many local minima and we might trap to one of those local minima and never reach to global minima so here it totally depends on our initialization of weights."
      ],
      "metadata": {
        "id": "85vFI0zlx7-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**solution for 2:**\n",
        "\n",
        "**observation** : In both the cases of increasing and decreasing the learning rate the time of convergence increases. And high value of learing rate doesnt guarantee that it will converge everytime."
      ],
      "metadata": {
        "id": "aYM6GUyAzSLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**solution for 3:**\n",
        "\n",
        "**Obseravtion:**\n",
        "\n",
        "Here we observe that in the polynomail equation of degree 5 there are more than local minima so we might not get everytime global minima with the random initialization. as it might trap in local minima or there might be a case that it might not converges so for convergence we should initialize the weights wisely. This was not in the case of quadratic equation as ther was only 1 minima ie. global minima so the initialization doesn't matter in that equation.\n"
      ],
      "metadata": {
        "id": "LQ3qdO1n8b-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. We can find the maxima instead of minima by two ways:**\n",
        "\n",
        "1.By multiplying with the -1 over the function through which we calulated minima.\n",
        "\n",
        "2.by adding the value of gradient descent to the weights each time like this\n",
        "\n",
        "![Screenshot 2023-01-04 190024.png](data:image/png;base64,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)\n"
      ],
      "metadata": {
        "id": "8XsjvNuO8iEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_e_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a second degree polynomial,\n",
        "    given a specific x value.\n",
        "    \n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, and c for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "    \n",
        "    Returns:\n",
        "        y: The corresponding output y value for the second degree polynomial.\n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * np.exp(x)\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    return y\n",
        "\n",
        "hundred_xs = np.random.uniform(-10,10, 100)\n",
        "coeffs = [1, 1, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_e_degree(coeffs, x)\n",
        "    xs.append(x)\n",
        "    ys.append(y)"
      ],
      "metadata": {
        "id": "nBbgQcIR-ugv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.ylim(-2, 50)\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xzXh-u5V_E3d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def eval_e_degree_(coeffs, x,j):\n",
        "\n",
        "    a = coeffs[0] * (x * x)\n",
        "    b = coeffs[1] * np.exp(x)\n",
        "    c = coeffs[2]\n",
        "    y = a + b + c\n",
        "    \n",
        "    interval = [y - j, y + j]\n",
        "    interval_min = interval[0]\n",
        "    interval_max = interval[1]\n",
        "    jit_val = random.random() * interval_max      # Generate a random number in range 0 to interval max \n",
        "    \n",
        "    while interval_min > jit_val:                 \n",
        "        jit_val = random.random() * interval_max  \n",
        "    \n",
        "    return jit_val"
      ],
      "metadata": {
        "id": "PFT1SSPU_QeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hundred_xs = np.random.uniform(-10, 10, 100)\n",
        "coeffs = [0, 1, 0]\n",
        "\n",
        "xs = []\n",
        "ys = []\n",
        "for x in hundred_xs:\n",
        "    y  = eval_e_degree_(coeffs, x,0.1)\n",
        "    xs.append(x)\n",
        "    ys.append(y)\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+')\n",
        "plt.ylim(-10, 50)\n",
        "plt.title('Original data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OruIDyXI_ikM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_coeffs = (random.randrange(-10, 10), random.randrange(-10, 10), random.randrange(-10, 10))\n",
        "y_bar = eval_e_degree(rand_coeffs, hundred_xs)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, y_bar, 'ro', label = 'prediction')\n",
        "plt.title('Original data vs first prediction')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.ylim(-20, 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "No20cIIA_8zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand_coeffs"
      ],
      "metadata": {
        "id": "WHz_LlqGAdSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradient_e_poly(rand_coeffs, hundred_xs, ys): \n",
        "    \n",
        "    \n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "    \n",
        "    y_bars = eval_e_degree(rand_coeffs, hundred_xs)\n",
        "    \n",
        "    for x, y, y_bar in list(zip(hundred_xs, ys, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2\n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = np.exp(x) * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    return (gradient_a, gradient_b, gradient_c)"
      ],
      "metadata": {
        "id": "B5hQxSssAyvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calc_grad = calc_gradient_e_poly(rand_coeffs, hundred_xs, ys)\n",
        "\n",
        "lr = 0.00000001\n",
        "a_new = rand_coeffs[0] - lr * calc_grad[0]\n",
        "b_new = rand_coeffs[1] - lr * calc_grad[1]\n",
        "c_new = rand_coeffs[2] - lr * calc_grad[2]\n",
        "\n",
        "new_model_coeffs = (a_new, b_new, c_new)\n",
        "print(f\"New model coeffs: {new_model_coeffs}\")\n",
        "\n",
        "# update with these new coeffs:\n",
        "new_y_bar = eval_e_degree(new_model_coeffs, hundred_xs)\n",
        "updated_model_loss = loss_mse(ys, new_y_bar)\n",
        "\n",
        "print(f\"Now have smaller model loss: {updated_model_loss} vs {initial_model_loss}\")\n",
        "print(calc_grad[0])\n",
        "print(calc_grad[1])\n",
        "print(calc_grad[2])"
      ],
      "metadata": {
        "id": "3v84_GXPBd-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(xs, ys, 'g+', label = 'original model')\n",
        "plt.plot(xs, y_bar, 'r.', label = 'first prediction')\n",
        "plt.plot(xs, new_y_bar, 'bo', label = 'updated prediction')\n",
        "plt.title('Original model vs 1st prediction vs updated prediction with lower loss')\n",
        "plt.legend(loc=\"upper left\")\n",
        "plt.ylim(-10, 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f-UWR3M2BzST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_gradient_e_poly_for_GD(coeffs, inputs_x, outputs_y, lr): \n",
        "    \n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "        \n",
        "    y_bars = eval_e_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x,y,y_bar in list(zip(inputs_x, outputs_y, y_bars)):    # take tuple of (x datapoint, actual y label, predicted y label)\n",
        "        x_squared = x ** 2        \n",
        "        partial_a = x_squared * (y - y_bar)\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = np.exp(x) * (y - y_bar)\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = (y - y_bar)\n",
        "        c_s.append(partial_c)\n",
        "    \n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "    \n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "\n",
        "    a_new = coeffs[0] - lr * gradient_a\n",
        "    b_new = coeffs[1] - lr * gradient_b\n",
        "    c_new = coeffs[2] - lr * gradient_c\n",
        "    \n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "    \n",
        "    # update with these new coeffs:\n",
        "    new_y_bar = eval_e_degree(new_model_coeffs, inputs_x)\n",
        "    \n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar"
      ],
      "metadata": {
        "id": "2hpjl1PYB1ys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(epochs, lr):\n",
        "    \n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_e_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses  # (updated_model_loss, new_model_coeffs, new_y_bar, saved loss updates)"
      ],
      "metadata": {
        "id": "D6A19b6lCfJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GD = gradient_descent(50000,  0.0000001)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(xs, ys, 'g+', label = 'original')\n",
        "plt.plot(xs, GD[2], 'b.', label = 'final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Descent')\n",
        "plt.legend(loc = \"lower right\")\n",
        "plt.ylim(-10,50)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Xsx9WNhVClSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final Coefficients predicted: {GD[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")"
      ],
      "metadata": {
        "id": "q2oAJNLkDFMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 6))\n",
        "plt.plot(GD[3], 'b-', label = 'loss')\n",
        "# plt.title('Loss over 500 iterations')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('MSE')\n",
        "plt.ylim(0, 10000)\n",
        "plt.xlim(0,10000)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s3T4xbV0AaeU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}